package searcher

import (
	"bytes"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
	"sort"
	"strings"
	"time"
)

// LocalAnalyzer provides AI-powered analysis using local LLM (Ollama)
type LocalAnalyzer struct {
	ollamaURL  string
	model      string
	timeout    time.Duration
	enabled    bool
	httpClient *http.Client
}

// AnalysisResult holds the result of AI analysis
type AnalysisResult struct {
	Summary          string             `json:"summary"`
	RiskAssessment   string             `json:"risk_assessment"`
	Recommendations  []string           `json:"recommendations"`
	CriticalFindings []CriticalFinding  `json:"critical_findings"`
	Statistics       AnalysisStatistics `json:"statistics"`
	AIInsights       string             `json:"ai_insights,omitempty"`
	ImageAnalyses    []ImageAIAnalysis  `json:"image_analyses,omitempty"`
	AnalyzedAt       string             `json:"analyzed_at"`
	UsedOllama       bool               `json:"used_ollama"`
}

// ImageAIAnalysis holds AI analysis result for a specific image
type ImageAIAnalysis struct {
	FilePath      string   `json:"file_path"`
	DocumentType  string   `json:"document_type"`
	Confidence    float64  `json:"confidence"`
	AIDescription string   `json:"ai_description"`
	RiskLevel     string   `json:"risk_level"`
	Warnings      []string `json:"warnings,omitempty"`
	DataFound     []string `json:"data_found,omitempty"`
}

// CriticalFinding represents a critical issue found
type CriticalFinding struct {
	FilePath    string  `json:"file_path"`
	Description string  `json:"description"`
	Severity    string  `json:"severity"`
	RiskScore   float64 `json:"risk_score"`
	Suggestion  string  `json:"suggestion"`
}

// AnalysisStatistics holds statistical analysis
type AnalysisStatistics struct {
	TotalFindings        int               `json:"total_findings"`
	UniqueFiles          int               `json:"unique_files"`
	AverageRiskScore     float64           `json:"average_risk_score"`
	MaxRiskScore         float64           `json:"max_risk_score"`
	SeverityDistribution map[string]int    `json:"severity_distribution"`
	PatternDistribution  map[string]int    `json:"pattern_distribution"`
	MostAffectedFiles    []FileRiskSummary `json:"most_affected_files"`
}

// FileRiskSummary summarizes risk for a single file
type FileRiskSummary struct {
	FilePath     string  `json:"file_path"`
	FindingCount int     `json:"finding_count"`
	MaxSeverity  string  `json:"max_severity"`
	AvgRiskScore float64 `json:"avg_risk_score"`
}

// NewLocalAnalyzer creates a new local analyzer
func NewLocalAnalyzer() *LocalAnalyzer {
	return &LocalAnalyzer{
		ollamaURL:  "http://localhost:11434",
		model:      "llama3.2", // Default model, can be changed
		timeout:    60 * time.Second,
		enabled:    false,
		httpClient: &http.Client{Timeout: 60 * time.Second},
	}
}

// SetModel sets the Ollama model to use
func (la *LocalAnalyzer) SetModel(model string) {
	la.model = model
}

// SetOllamaURL sets the Ollama API URL
func (la *LocalAnalyzer) SetOllamaURL(url string) {
	la.ollamaURL = url
}

// EnableAI enables AI analysis
func (la *LocalAnalyzer) EnableAI(enabled bool) {
	la.enabled = enabled
}

// IsOllamaAvailable checks if Ollama is running
func (la *LocalAnalyzer) IsOllamaAvailable() bool {
	resp, err := la.httpClient.Get(la.ollamaURL + "/api/tags")
	if err != nil {
		return false
	}
	defer resp.Body.Close()
	return resp.StatusCode == 200
}

// GetAvailableModels returns list of available Ollama models
func (la *LocalAnalyzer) GetAvailableModels() ([]string, error) {
	resp, err := la.httpClient.Get(la.ollamaURL + "/api/tags")
	if err != nil {
		return nil, fmt.Errorf("ollama Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½: %v", err)
	}
	defer resp.Body.Close()

	var result struct {
		Models []struct {
			Name string `json:"name"`
		} `json:"models"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return nil, err
	}

	var models []string
	for _, m := range result.Models {
		models = append(models, m.Name)
	}
	return models, nil
}

// Analyze performs comprehensive analysis of scan results
func (la *LocalAnalyzer) Analyze(result *ScanResult) (*AnalysisResult, error) {
	analysis := &AnalysisResult{
		AnalyzedAt: time.Now().Format("02.01.2006 15:04:05"),
		UsedOllama: false,
	}

	// Calculate statistics
	analysis.Statistics = la.calculateStatistics(result)

	// Generate rule-based analysis
	analysis.Summary = la.generateSummary(result, &analysis.Statistics)
	analysis.RiskAssessment = la.assessRisk(result, &analysis.Statistics)
	analysis.Recommendations = la.generateRecommendations(result, &analysis.Statistics)
	analysis.CriticalFindings = la.identifyCriticalFindings(result)

	// If AI is enabled and Ollama is available, get AI insights
	if la.enabled && la.IsOllamaAvailable() {
		analysis.UsedOllama = true

		// Get text-based AI insights
		insights, err := la.getAIInsights(result, &analysis.Statistics)
		if err == nil {
			analysis.AIInsights = insights
		}

		// Analyze document images with AI
		analysis.ImageAnalyses = la.analyzeDocumentImages(result)
	}

	return analysis, nil
}

// analyzeDocumentImages analyzes detected document images with AI
func (la *LocalAnalyzer) analyzeDocumentImages(result *ScanResult) []ImageAIAnalysis {
	var analyses []ImageAIAnalysis

	for _, f := range result.Findings {
		// Only analyze image-based document findings
		if f.PatternType != PatternPassport {
			continue
		}

		// Check if it's an image file
		filePath := f.FilePath
		isImage := strings.HasSuffix(strings.ToLower(filePath), ".jpg") ||
			strings.HasSuffix(strings.ToLower(filePath), ".jpeg") ||
			strings.HasSuffix(strings.ToLower(filePath), ".png")

		if !isImage {
			// For PDF findings, create analysis based on existing data
			if strings.HasSuffix(strings.ToLower(filePath), ".pdf") {
				analysis := ImageAIAnalysis{
					FilePath:      filePath,
					DocumentType:  f.MatchedText,
					Confidence:    f.RiskScore,
					RiskLevel:     la.getRiskLevelFromScore(f.RiskScore),
					AIDescription: la.generateDocumentDescription(f),
					Warnings:      la.generateWarnings(f),
					DataFound:     la.extractFoundData(f),
				}
				analyses = append(analyses, analysis)
			}
			continue
		}

		// Try to analyze with vision model (llava)
		aiDesc := la.analyzeImageWithVision(filePath)

		analysis := ImageAIAnalysis{
			FilePath:      filePath,
			DocumentType:  f.MatchedText,
			Confidence:    f.RiskScore,
			RiskLevel:     la.getRiskLevelFromScore(f.RiskScore),
			AIDescription: aiDesc,
			Warnings:      la.generateWarnings(f),
			DataFound:     la.extractFoundData(f),
		}
		analyses = append(analyses, analysis)
	}

	return analyses
}

// analyzeImageWithVision uses Ollama vision model to analyze image
func (la *LocalAnalyzer) analyzeImageWithVision(imagePath string) string {
	// Check if we have a vision model (llava, bakllava, etc.)
	models, err := la.GetAvailableModels()
	if err != nil {
		return la.getDefaultImageDescription()
	}

	// Look for vision model
	visionModel := ""
	for _, m := range models {
		if strings.Contains(strings.ToLower(m), "llava") ||
			strings.Contains(strings.ToLower(m), "bakllava") ||
			strings.Contains(strings.ToLower(m), "moondream") {
			visionModel = m
			break
		}
	}

	if visionModel == "" {
		// No vision model available, use text description
		return la.getDefaultImageDescription()
	}

	// Read and encode image
	imageData, err := la.encodeImageBase64(imagePath)
	if err != nil {
		return la.getDefaultImageDescription()
	}

	// Call Ollama with vision model
	prompt := `ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ ÑÑ‚Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°. ĞĞ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¾Ğ¿Ğ¸ÑˆĞ¸:
1. Ğ§Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ·Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ (Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚, ÑƒĞ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€Ğ°Ğ²Ğ° Ğ¸ Ñ‚.Ğ´.)
2. ĞšĞ°ĞºĞ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ½Ğ° (Ğ±ĞµĞ· ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
3. Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ¸ÑĞºĞ° ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
Ğ‘ÑƒĞ´ÑŒ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¼, 2-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ.`

	reqBody := map[string]interface{}{
		"model":  visionModel,
		"prompt": prompt,
		"images": []string{imageData},
		"stream": false,
		"options": map[string]interface{}{
			"temperature": 0.3,
			"num_predict": 200,
		},
	}

	jsonBody, _ := json.Marshal(reqBody)

	resp, err := la.httpClient.Post(
		la.ollamaURL+"/api/generate",
		"application/json",
		bytes.NewReader(jsonBody),
	)
	if err != nil {
		return la.getDefaultImageDescription()
	}
	defer resp.Body.Close()

	body, _ := io.ReadAll(resp.Body)

	var ollamaResp struct {
		Response string `json:"response"`
	}

	if err := json.Unmarshal(body, &ollamaResp); err != nil {
		return la.getDefaultImageDescription()
	}

	if ollamaResp.Response == "" {
		return la.getDefaultImageDescription()
	}

	return ollamaResp.Response
}

// encodeImageBase64 reads and encodes image to base64
func (la *LocalAnalyzer) encodeImageBase64(imagePath string) (string, error) {
	// Read actual file and encode to base64
	return readImageFile(imagePath)
}

// readImageFile reads image file and returns base64
func readImageFile(path string) (string, error) {
	data, err := os.ReadFile(path)
	if err != nil {
		return "", err
	}
	return base64.StdEncoding.EncodeToString(data), nil
}

// getDefaultImageDescription returns default description when AI unavailable
func (la *LocalAnalyzer) getDefaultImageDescription() string {
	return "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚, ÑƒĞ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€ÑÑÑ‰Ğ¸Ğ¹ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¸ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ¸Ğ· Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°."
}

// getRiskLevelFromScore converts risk score to text level
func (la *LocalAnalyzer) getRiskLevelFromScore(score float64) string {
	if score >= 80 {
		return "ğŸ”´ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™"
	} else if score >= 60 {
		return "ğŸŸ  Ğ’Ğ«Ğ¡ĞĞšĞ˜Ğ™"
	} else if score >= 40 {
		return "ğŸŸ¡ Ğ¡Ğ Ğ•Ğ”ĞĞ˜Ğ™"
	}
	return "ğŸŸ¢ ĞĞ˜Ğ—ĞšĞ˜Ğ™"
}

// generateDocumentDescription generates description based on finding
func (la *LocalAnalyzer) generateDocumentDescription(f *Finding) string {
	docType := f.MatchedText

	descriptions := map[string]string{
		"passport_page":   "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†Ğ°.",
		"passport_closed": "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚ (Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚). Ğ’Ğ¸Ğ´ĞµĞ½ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ñ†ĞµĞ»Ğ¸ĞºĞ¾Ğ¼.",
		"passport_card":   "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ñ Ñ„Ğ¾Ñ‚Ğ¾ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.",
		"id_card":         "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ ÑƒĞ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹.",
		"driver_license":  "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ ÑƒĞ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ¸Ğµ.",
		"passport":        "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.",
	}

	if desc, ok := descriptions[docType]; ok {
		return desc
	}
	return "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ."
}

// generateWarnings generates warnings based on finding
func (la *LocalAnalyzer) generateWarnings(f *Finding) []string {
	var warnings []string

	if f.RiskScore >= 80 {
		warnings = append(warnings, "âš ï¸ Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ Ñ€Ğ¸ÑĞº ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
	}
	if f.RiskScore >= 60 {
		warnings = append(warnings, "âš ï¸ Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ")
	}

	// Document-specific warnings
	docType := f.MatchedText
	if strings.Contains(docType, "passport") {
		warnings = append(warnings, "âš ï¸ ĞŸĞ°ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°")
	}

	return warnings
}

// extractFoundData extracts what data was found
func (la *LocalAnalyzer) extractFoundData(f *Finding) []string {
	var data []string

	docType := f.MatchedText

	if strings.Contains(docType, "passport") {
		data = append(data, "ğŸ“‹ Ğ¤Ğ˜Ğ Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†Ğ°")
		data = append(data, "ğŸ“… Ğ”Ğ°Ñ‚Ğ° Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ")
		data = append(data, "ğŸ”¢ ĞĞ¾Ğ¼ĞµÑ€ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°")
		data = append(data, "ğŸ“¸ Ğ¤Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ")
	}

	if strings.Contains(docType, "id_card") || strings.Contains(docType, "driver") {
		data = append(data, "ğŸ“‹ ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ")
		data = append(data, "ğŸ“¸ Ğ¤Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ")
		data = append(data, "ğŸ”¢ ĞĞ¾Ğ¼ĞµÑ€ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°")
	}

	if len(data) == 0 {
		data = append(data, "ğŸ“‹ ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ")
	}

	return data
}

// calculateStatistics calculates detailed statistics
func (la *LocalAnalyzer) calculateStatistics(result *ScanResult) AnalysisStatistics {
	stats := AnalysisStatistics{
		TotalFindings:        len(result.Findings),
		SeverityDistribution: make(map[string]int),
		PatternDistribution:  make(map[string]int),
	}

	fileFindings := make(map[string][]Finding)
	var totalRisk float64
	stats.MaxRiskScore = 0

	for _, f := range result.Findings {
		// Severity distribution
		stats.SeverityDistribution[string(f.Severity)]++

		// Pattern distribution
		stats.PatternDistribution[string(f.PatternType)]++

		// Risk scores
		totalRisk += f.RiskScore
		if f.RiskScore > stats.MaxRiskScore {
			stats.MaxRiskScore = f.RiskScore
		}

		// Group by file
		fileFindings[f.FilePath] = append(fileFindings[f.FilePath], *f)
	}

	stats.UniqueFiles = len(fileFindings)

	if len(result.Findings) > 0 {
		stats.AverageRiskScore = totalRisk / float64(len(result.Findings))
	}

	// Calculate most affected files
	for filePath, findings := range fileFindings {
		var maxSeverity Severity = Low
		var totalFileRisk float64

		for _, f := range findings {
			totalFileRisk += f.RiskScore
			if f.Severity.Score() > maxSeverity.Score() {
				maxSeverity = f.Severity
			}
		}

		stats.MostAffectedFiles = append(stats.MostAffectedFiles, FileRiskSummary{
			FilePath:     filePath,
			FindingCount: len(findings),
			MaxSeverity:  string(maxSeverity),
			AvgRiskScore: totalFileRisk / float64(len(findings)),
		})
	}

	// Sort by finding count
	sort.Slice(stats.MostAffectedFiles, func(i, j int) bool {
		return stats.MostAffectedFiles[i].FindingCount > stats.MostAffectedFiles[j].FindingCount
	})

	// Keep only top 10
	if len(stats.MostAffectedFiles) > 10 {
		stats.MostAffectedFiles = stats.MostAffectedFiles[:10]
	}

	return stats
}

// generateSummary generates a human-readable summary
func (la *LocalAnalyzer) generateSummary(result *ScanResult, stats *AnalysisStatistics) string {
	var sb strings.Builder

	sb.WriteString(fmt.Sprintf("ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» %d Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ‚ĞµÑ‡ĞµĞº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² %d Ñ„Ğ°Ğ¹Ğ»Ğ°Ñ….\n\n",
		stats.TotalFindings, stats.UniqueFiles))

	critCount := stats.SeverityDistribution[string(Critical)]
	highCount := stats.SeverityDistribution[string(High)]

	if critCount > 0 || highCount > 0 {
		sb.WriteString(fmt.Sprintf("âš ï¸ Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ %d ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ %d ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ½ĞµĞ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.\n\n",
			critCount, highCount))
	}

	// Top patterns
	type patternCount struct {
		pattern string
		count   int
	}
	var patterns []patternCount
	for p, c := range stats.PatternDistribution {
		patterns = append(patterns, patternCount{p, c})
	}
	sort.Slice(patterns, func(i, j int) bool {
		return patterns[i].count > patterns[j].count
	})

	sb.WriteString("ĞĞ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ğ°ÑÑ‚Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ ÑƒÑ‚ĞµÑ‡ĞµĞº:\n")
	for i, p := range patterns {
		if i >= 5 {
			break
		}
		sb.WriteString(fmt.Sprintf("  â€¢ %s: %d\n", patternTypeToRussian(PatternType(p.pattern)), p.count))
	}

	return sb.String()
}

// assessRisk provides risk assessment
func (la *LocalAnalyzer) assessRisk(result *ScanResult, stats *AnalysisStatistics) string {
	var level string
	var description string

	critCount := stats.SeverityDistribution[string(Critical)]
	highCount := stats.SeverityDistribution[string(High)]

	switch {
	case critCount >= 10 || (critCount >= 5 && highCount >= 10):
		level = "ğŸ”´ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™"
		description = "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑ‚ĞµÑ‡ĞµĞº. Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ½ĞµĞ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾. " +
			"Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ´ĞµĞ¿Ğ»Ğ¾Ğ¹ Ğ´Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼."

	case critCount >= 3 || highCount >= 10:
		level = "ğŸŸ  Ğ’Ğ«Ğ¡ĞĞšĞ˜Ğ™"
		description = "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ " +
			"ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ¾Ğ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ´Ğ°ĞºÑˆĞ½."

	case critCount >= 1 || highCount >= 5:
		level = "ğŸŸ¡ Ğ¡Ğ Ğ•Ğ”ĞĞ˜Ğ™"
		description = "ĞŸÑ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ Ñ€ĞµĞ²ÑŒÑ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… " +
			"Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ñ… Ğ² Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ."

	case highCount >= 1 || stats.TotalFindings >= 10:
		level = "ğŸŸ¡ Ğ£ĞœĞ•Ğ Ğ•ĞĞĞ«Ğ™"
		description = "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ·Ğ°Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ."

	default:
		level = "ğŸŸ¢ ĞĞ˜Ğ—ĞšĞ˜Ğ™"
		description = "ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³."
	}

	return fmt.Sprintf("Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ¸ÑĞºĞ°: %s\n\n%s\n\nĞ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ±Ğ°Ğ»Ğ» Ñ€Ğ¸ÑĞºĞ°: %.1f / 100\nĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ»: %.1f / 100",
		level, description, stats.AverageRiskScore, stats.MaxRiskScore)
}

// generateRecommendations provides actionable recommendations
func (la *LocalAnalyzer) generateRecommendations(result *ScanResult, stats *AnalysisStatistics) []string {
	var recs []string

	critCount := stats.SeverityDistribution[string(Critical)]
	highCount := stats.SeverityDistribution[string(High)]

	// Critical recommendations
	if critCount > 0 {
		recs = append(recs, "ğŸ”´ ĞĞµĞ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚Ğµ Ğ¸Ğ»Ğ¸ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ²ÑĞµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ Ğ¸Ğ· ĞºĞ¾Ğ´Ğ°")
	}

	// Pattern-specific recommendations
	if stats.PatternDistribution[string(PatternPrivateKey)] > 0 {
		recs = append(recs, "ğŸ”‘ ĞŸĞµÑ€ĞµĞ¼ĞµÑÑ‚Ğ¸Ñ‚Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡Ğ¸ Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ (HashiCorp Vault, AWS Secrets Manager)")
	}

	if stats.PatternDistribution[string(PatternPassword)] > 0 {
		recs = append(recs, "ğŸ” Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ ÑĞµĞºÑ€ĞµÑ‚Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ…Ğ°Ñ€Ğ´ĞºĞ¾Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ¾Ğ»ĞµĞ¹")
	}

	if stats.PatternDistribution[string(PatternAPIKey)] > 0 || stats.PatternDistribution[string(PatternAWSKey)] > 0 {
		recs = append(recs, "ğŸ—ï¸ Ğ Ğ¾Ñ‚Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ²ÑĞµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğµ API-ĞºĞ»ÑÑ‡Ğ¸ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹Ñ‚Ğµ Ğ¸Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ")
	}

	if stats.PatternDistribution[string(PatternCreditCard)] > 0 {
		recs = append(recs, "ğŸ’³ Ğ¡Ğ ĞĞ§ĞĞ: Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ½ĞºĞ¾Ğ²ÑĞºĞ¸Ñ… ĞºĞ°Ñ€Ñ‚ Ğ¸Ğ· ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹. Ğ­Ñ‚Ğ¾ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğµ PCI DSS!")
	}

	if stats.PatternDistribution[string(PatternConnectionStr)] > 0 {
		recs = append(recs, "ğŸ”— Ğ’Ñ‹Ğ½ĞµÑĞ¸Ñ‚Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ‘Ğ” Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ")
	}

	// General recommendations
	if len(result.Findings) > 0 {
		recs = append(recs, "ğŸ“‹ Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ pre-commit Ñ…ÑƒĞº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞµĞºÑ€ĞµÑ‚Ğ¾Ğ²")
		recs = append(recs, "ğŸ›¡ï¸ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹Ñ‚Ğµ .gitignore Ğ´Ğ»Ñ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ ÑĞµĞºÑ€ĞµÑ‚Ğ°Ğ¼Ğ¸")
		recs = append(recs, "ğŸ“ ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ .env.example Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… (Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹)")
	}

	if critCount > 0 || highCount > 0 {
		recs = append(recs, "ğŸ”„ ĞŸĞ¾ÑĞ»Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸")
	}

	return recs
}

// identifyCriticalFindings identifies most critical issues
func (la *LocalAnalyzer) identifyCriticalFindings(result *ScanResult) []CriticalFinding {
	var critical []CriticalFinding

	for _, f := range result.Findings {
		if f.Severity == Critical || (f.Severity == High && f.RiskScore >= 70) {
			suggestion := la.getSuggestionForPattern(f.PatternType)

			critical = append(critical, CriticalFinding{
				FilePath:    f.FilePath,
				Description: descriptionToRussian(f.Description),
				Severity:    severityToRussian(f.Severity),
				RiskScore:   f.RiskScore,
				Suggestion:  suggestion,
			})
		}
	}

	// Sort by risk score
	sort.Slice(critical, func(i, j int) bool {
		return critical[i].RiskScore > critical[j].RiskScore
	})

	// Limit to top 20
	if len(critical) > 20 {
		critical = critical[:20]
	}

	return critical
}

// getSuggestionForPattern returns a suggestion for fixing a specific pattern
func (la *LocalAnalyzer) getSuggestionForPattern(pattern PatternType) string {
	suggestions := map[PatternType]string{
		PatternPassword:      "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ: os.Getenv(\"DB_PASSWORD\")",
		PatternAPIKey:        "Ğ¥Ñ€Ğ°Ğ½Ğ¸Ñ‚Ğµ Ğ² .env Ñ„Ğ°Ğ¹Ğ»Ğµ: API_KEY=xxx, Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ğ¹Ñ‚Ğµ Ñ‡ĞµÑ€ĞµĞ· godotenv",
		PatternPrivateKey:    "ĞŸĞµÑ€ĞµĞ¼ĞµÑÑ‚Ğ¸Ñ‚Ğµ Ğ² ~/.ssh/ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Vault Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ",
		PatternAWSKey:        "ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹Ñ‚Ğµ AWS credentials Ñ‡ĞµÑ€ĞµĞ· aws configure Ğ¸Ğ»Ğ¸ IAM Ñ€Ğ¾Ğ»Ğ¸",
		PatternGitHubToken:   "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ GitHub App Ğ¸Ğ»Ğ¸ Personal Access Token Ğ² ÑĞµĞºÑ€ĞµÑ‚Ğ°Ñ… CI/CD",
		PatternCreditCard:    "ĞĞ•ĞœĞ•Ğ”Ğ›Ğ•ĞĞĞ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚Ğµ! Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚ Ğ½Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒÑÑ Ğ² ĞºĞ¾Ğ´Ğµ",
		PatternConnectionStr: "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ DATABASE_URL Ğ¸Ğ· Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ",
		PatternToken:         "Ğ¥Ñ€Ğ°Ğ½Ğ¸Ñ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ¼ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ ÑĞµĞºÑ€ĞµÑ‚Ğ¾Ğ²",
	}

	if suggestion, ok := suggestions[pattern]; ok {
		return suggestion
	}
	return "Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚Ğµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ"
}

// getAIInsights gets AI-powered insights from Ollama
func (la *LocalAnalyzer) getAIInsights(result *ScanResult, stats *AnalysisStatistics) (string, error) {
	// Build prompt
	prompt := la.buildAnalysisPrompt(result, stats)

	// Call Ollama
	reqBody := map[string]interface{}{
		"model":  la.model,
		"prompt": prompt,
		"stream": false,
		"options": map[string]interface{}{
			"temperature": 0.3,
			"num_predict": 500,
		},
	}

	jsonBody, _ := json.Marshal(reqBody)

	resp, err := la.httpClient.Post(
		la.ollamaURL+"/api/generate",
		"application/json",
		bytes.NewReader(jsonBody),
	)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	body, _ := io.ReadAll(resp.Body)

	var ollamaResp struct {
		Response string `json:"response"`
	}

	if err := json.Unmarshal(body, &ollamaResp); err != nil {
		return "", err
	}

	return ollamaResp.Response, nil
}

// buildAnalysisPrompt builds the prompt for AI analysis
func (la *LocalAnalyzer) buildAnalysisPrompt(result *ScanResult, stats *AnalysisStatistics) string {
	var sb strings.Builder

	sb.WriteString("Ğ¢Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.\n\n")

	sb.WriteString("Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°:\n")
	sb.WriteString(fmt.Sprintf("- Ğ’ÑĞµĞ³Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¾Ğº: %d\n", stats.TotalFindings))
	sb.WriteString(fmt.Sprintf("- ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…: %d\n", stats.SeverityDistribution[string(Critical)]))
	sb.WriteString(fmt.Sprintf("- Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ñ…: %d\n", stats.SeverityDistribution[string(High)]))
	sb.WriteString(fmt.Sprintf("- Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ñ…: %d\n", stats.SeverityDistribution[string(Medium)]))
	sb.WriteString(fmt.Sprintf("- Ğ—Ğ°Ñ‚Ñ€Ğ¾Ğ½ÑƒÑ‚Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: %d\n", stats.UniqueFiles))
	sb.WriteString(fmt.Sprintf("- Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ€Ğ¸ÑĞº: %.1f\n\n", stats.AverageRiskScore))

	sb.WriteString("Ğ¢Ğ¸Ğ¿Ñ‹ ÑƒÑ‚ĞµÑ‡ĞµĞº:\n")
	for pattern, count := range stats.PatternDistribution {
		sb.WriteString(fmt.Sprintf("- %s: %d\n", pattern, count))
	}

	sb.WriteString("\nĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¾Ğº (Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 5):\n")
	shown := 0
	for _, f := range result.Findings {
		if f.Severity == Critical && shown < 5 {
			sb.WriteString(fmt.Sprintf("- %s:%d - %s\n", f.FilePath, f.LineNumber, f.Description))
			shown++
		}
	}

	sb.WriteString("\nĞ”Ğ°Ğ¹ 3-5 ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºÑƒ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼.")

	return sb.String()
}

// FormatAnalysisReport formats analysis result as a readable report
func (la *LocalAnalyzer) FormatAnalysisReport(analysis *AnalysisResult) string {
	var sb strings.Builder

	sb.WriteString("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n")
	sb.WriteString("â•‘             ĞĞ¢Ğ§ĞĞ¢ ĞĞĞĞ›Ğ˜Ğ—Ğ Ğ‘Ğ•Ğ—ĞĞŸĞĞ¡ĞĞĞ¡Ğ¢Ğ˜                       â•‘\n")
	sb.WriteString("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")

	sb.WriteString(fmt.Sprintf("Ğ”Ğ°Ñ‚Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: %s\n\n", analysis.AnalyzedAt))

	sb.WriteString("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
	sb.WriteString("Ğ¡Ğ’ĞĞ”ĞšĞ\n")
	sb.WriteString("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
	sb.WriteString(analysis.Summary)
	sb.WriteString("\n")

	sb.WriteString("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
	sb.WriteString("ĞĞ¦Ğ•ĞĞšĞ Ğ Ğ˜Ğ¡ĞšĞ\n")
	sb.WriteString("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
	sb.WriteString(analysis.RiskAssessment)
	sb.WriteString("\n\n")

	if len(analysis.CriticalFindings) > 0 {
		sb.WriteString("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
		sb.WriteString("ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• ĞĞĞ¥ĞĞ”ĞšĞ˜\n")
		sb.WriteString("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
		for i, cf := range analysis.CriticalFindings {
			sb.WriteString(fmt.Sprintf("\n%d. %s\n", i+1, cf.FilePath))
			sb.WriteString(fmt.Sprintf("   ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ: %s\n", cf.Description))
			sb.WriteString(fmt.Sprintf("   Ğ¡ĞµÑ€ÑŒÑ‘Ğ·Ğ½Ğ¾ÑÑ‚ÑŒ: %s | Ğ Ğ¸ÑĞº: %.0f%%\n", cf.Severity, cf.RiskScore))
			sb.WriteString(fmt.Sprintf("   ğŸ’¡ %s\n", cf.Suggestion))
		}
		sb.WriteString("\n")
	}

	sb.WriteString("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
	sb.WriteString("Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ¦Ğ˜Ğ˜\n")
	sb.WriteString("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
	for i, rec := range analysis.Recommendations {
		sb.WriteString(fmt.Sprintf("%d. %s\n", i+1, rec))
	}

	if analysis.AIInsights != "" {
		sb.WriteString("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
		sb.WriteString("ğŸ¤– AI-ĞĞĞĞ›Ğ˜Ğ— (Ollama)\n")
		sb.WriteString("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
		sb.WriteString(analysis.AIInsights)
		sb.WriteString("\n")
	}

	sb.WriteString("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

	return sb.String()
}
